{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import timm\n",
    "import random\n",
    "import csv   \n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import configparser\n",
    "import ast\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "sys.path.append('../training')  # This adds the parent directory to the system path\n",
    "from utils import *\n",
    "from dataloaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset_v2(model, criterion, loader, input_data, device, batch_size, export_preds=False):\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        if device != 'cpu':\n",
    "            net =  (model) if batch_size > 10 else model\n",
    "        else:\n",
    "            net = model\n",
    "        for i, data in enumerate(loader):\n",
    "            inputs, fixation, labels = data # get the inputs; data is a list of [inputs, fixation, labels]\n",
    "            all_labels.extend(list(labels.numpy()))\n",
    "            inputs = inputs.to(device)\n",
    "            fixation = fixation.to(device)\n",
    "            labels = labels.to(device) \n",
    "\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            total_loss += criterion(outputs, labels).item() * batch_size\n",
    "            \n",
    "            if export_preds:\n",
    "                all_probs.extend(outputs.data.cpu().detach().numpy())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if export_preds:\n",
    "                all_preds.extend(predicted.cpu().detach().numpy())\n",
    "            correct += (predicted == labels).sum()\n",
    "        accuracy = 100 * (correct.item()) / len(input_data)\n",
    "    \n",
    "    return np.array(all_labels), np.array(all_probs), np.array(all_preds), accuracy, total_loss\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    # Assuming y_true and y_pred are your arrays of true labels and predictions respectively\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "    # ROC-AUC score calculation for binary classification\n",
    "    # For multi-class, you need the prediction scores, not the predicted labels, and use a different strategy\n",
    "    roc_auc = roc_auc_score(y_true, y_pred) if len(set(y_true)) == 2 else \"ROC-AUC not applicable\"\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    # print(f\"Precision: {precision}\")\n",
    "    # print(f\"Recall: {recall}\")\n",
    "    # print(f\"F1 Score: {f1}\")\n",
    "    # print(f\"ROC-AUC Score: {roc_auc}\")\n",
    "    # print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1, roc_auc, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "model_type      = 'vit'\n",
    "dataset_type    = 'dreyeve'\n",
    "subset_layers   = 5\n",
    "\n",
    "if model_type == 'vit':\n",
    "    lambda_value = None\n",
    "elif model_type == 'fax':\n",
    "    lambda_value    = 0.1 # 0.01, 0.1, 0.2, 0.8, 1\n",
    "\n",
    "train_data_cond = 'full'\n",
    "test_data_cond  = 'full'\n",
    "runs            = 10\n",
    "\n",
    "data_path       = 'fixatt/data' # path to datasets\n",
    "vit_version     = 'vit_base_patch16_224'\n",
    "model_load_type = 'early_stopped'\n",
    "validation      = True\n",
    "device          = torch.device(\"cuda:1\")\n",
    "\n",
    "if dataset_type == 'vr':\n",
    "    batch_size = 64\n",
    "elif dataset_type == 'dreyeve':\n",
    "    batch_size = 16\n",
    "\n",
    "print(f'model_type: {model_type}, dataset_type: {dataset_type}, subset_layers: {subset_layers}, lambda_value: {lambda_value}')\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics for each run\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "\n",
    "for run in range(runs):\n",
    "    # Define parameters\n",
    "    random_state = run\n",
    "    if random_state:\n",
    "        torch.manual_seed(random_state)\n",
    "        random.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Load data\n",
    "    train_list, valid_list, test_list = get_split_data(dataset_type, data_path, train_data_cond, dataset_type, validation, random_state)\n",
    "    train_loader, valid_loader, test_loader, train_data, valid_data, test_data = get_loaders(dataset_type, model_type, train_list, valid_list, test_list, \n",
    "                                                                                            batch_size, train_data_cond, test_data_cond)\n",
    "\n",
    "    # Load model and loss function\n",
    "    pretrained_path = f'fixatt/training/pre_trained_models/{dataset_type}_{test_data_cond}_{model_type}'    \n",
    "    pretrained_weights = f'{pretrained_path}/trained_LR_driving_{run}_{subset_layers}_layer_{lambda_value}_lambda_{model_load_type}.pt'\n",
    "    model = timm.create_model(vit_version, num_classes=2).to(device)\n",
    "    if subset_layers:\n",
    "        model.blocks = nn.Sequential(*[model.blocks[i] for i in range(subset_layers)])\n",
    "    model.load_state_dict(torch.load(pretrained_weights, map_location=device))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Inference using test set\n",
    "    all_labels, all_probs, all_preds, test_accuracy, test_total_loss = eval_dataset_v2(model, criterion, test_loader, test_data, device, batch_size, export_preds=True)\n",
    "\n",
    "    accuracy, precision, recall, f1, roc_auc, conf_matrix = get_metrics(all_labels, all_preds)\n",
    "\n",
    "    # Store the metrics\n",
    "    accuracies.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "# Compute mean and std for each metric\n",
    "accuracy_mean = np.mean(accuracies)\n",
    "accuracy_std = np.std(accuracies)\n",
    "\n",
    "f1_mean = np.mean(f1_scores)\n",
    "f1_std = np.std(f1_scores)\n",
    "\n",
    "roc_auc_mean = np.mean(roc_auc_scores)\n",
    "roc_auc_std = np.std(roc_auc_scores)\n",
    "\n",
    "# Print the mean ± std for accuracy, f1, and roc_auc scores\n",
    "print(f'Accuracy: {accuracy_mean:.2f} ± {accuracy_std:.2f}')\n",
    "print(f'ROC AUC Score: {roc_auc_mean:.2f} ± {roc_auc_std:.2f}')\n",
    "print(f'F1 Score: {f1_mean:.2f} ± {f1_std:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies_from_csv(dataset, main_path):\n",
    "    # Initialize variables for accumulating accuracies and counting the runs\n",
    "    total_accuracy = 0\n",
    "    count_runs = 0\n",
    "    best_run = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Loop through all possible runs\n",
    "    for run in range(10):  # Assuming runs from 0 to 9\n",
    "        csv_file_name = f\"trained_LR_driving_{run}_{subset_layers}_layer_{lambda_value}_lambda.csv\"\n",
    "            \n",
    "        csv_file_path = Path(main_path) / csv_file_name\n",
    "        \n",
    "        # Check if the CSV file exists\n",
    "        if csv_file_path.exists():\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            accuracy = df.iloc[-20]['test_accuracy']\n",
    "            print(f'Accuracy for {dataset}, run {run}: {accuracy:.2f}')\n",
    "            \n",
    "            # Accumulate total accuracy and increment run count\n",
    "            total_accuracy += accuracy\n",
    "            count_runs += 1\n",
    "            \n",
    "            # Update the best run if this run has a higher accuracy\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_run = run\n",
    "\n",
    "    # Calculate the average accuracy if there were any runs\n",
    "    if count_runs > 0:\n",
    "        average_accuracy = total_accuracy / count_runs\n",
    "        print(f\"For {dataset}: The average accuracy is {average_accuracy:.2f}\")\n",
    "    else:\n",
    "        print(f\"No runs found for {dataset}\")\n",
    "\n",
    "    # Print the best run and its accuracy\n",
    "    print(f\"For {dataset}: The best run is {best_run} with an accuracy of {best_accuracy:.2f}\")\n",
    "\n",
    "main_path = f'fixatt/training/pre_trained_models/{dataset_type}_full_{model_type}'\n",
    "plot_accuracies_from_csv(dataset_type, main_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnt_no_timm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
