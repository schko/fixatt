{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary step**\n",
    "\n",
    "If you want to train ViT with FAX loss, you need to change the code ViT code inside timm library (`https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py`) with the code in the file `fixatt/training/vision_transformer_modified_for_fax_loss.py` and load the modified timm library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import timm\n",
    "import random\n",
    "import csv   \n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from utils import *\n",
    "from dataloaders import *\n",
    "\n",
    "import os\n",
    "import configparser\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "# Define a function to convert string to boolean\n",
    "def str_to_bool(s):\n",
    "    return s.lower() == 'true'\n",
    "\n",
    "# Get the parent directory path\n",
    "parent_directory = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "# Construct the path to the config.ini file in the parent directory\n",
    "config_file_path = os.path.join(parent_directory, '..', 'config.ini')\n",
    "\n",
    "# Load the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file_path)\n",
    "\n",
    "# Access configuration parameters\n",
    "model_type = config['model_config']['model_type']\n",
    "model_load_type =  config['model_config']['model_load_type']\n",
    "subset_layers =  ast.literal_eval(config['model_config']['subset_layers'])\n",
    "vit_version = config['model_config']['vit_version']\n",
    "\n",
    "dataset_type = config['data']['dataset_type']\n",
    "train_data_cond = config['data']['train_data_cond']\n",
    "test_data_cond = config['data']['test_data_cond']\n",
    "\n",
    "validation = str_to_bool(config['hyperparams']['validation'])\n",
    "batch_size = int(config['hyperparams'][f'batch_size_{dataset_type}'])\n",
    "early_stopping_epochs = int(config['hyperparams']['early_stopping_epochs'])\n",
    "runs = int(config['hyperparams']['runs'])\n",
    "\n",
    "data_path = config['paths']['global_data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define parameters'''\n",
    "\n",
    "train = True # retrain?\n",
    "trained_model_path = f'pre_trained_models/{dataset_type}_{train_data_cond}_{model_type}/trained_LR_driving' # default pre_trained_models/full_vit | random_peripheral_vit | .../trained_LR_driving\n",
    "Path(f'pre_trained_models/{dataset_type}_{train_data_cond}_{model_type}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('device:', device)\n",
    "print('trained_model_path:', trained_model_path)\n",
    "print('subset_layers:', subset_layers)\n",
    "\n",
    "# Re-run peripheral data ectraction\n",
    "re_run_peripheral_data_extraction = False\n",
    "if re_run_peripheral_data_extraction:\n",
    "    peripheral_data_extraction(dataset_type, train_data_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'fax':\n",
    "    # lambdas = [0.01, 0.1, 0.2, 0.8, 1] # different values for the hyperparameter $\\lambda$ of the FAX loss\n",
    "    lambdas = [1]\n",
    "else:\n",
    "    lambdas = [None]\n",
    "\n",
    "# Redefine runs in case you want to train for specific run numbers\n",
    "# runs = [5, 6, 7, 8, 9]\n",
    "runs = list(range(10))\n",
    "\n",
    "\n",
    "# Training\n",
    "if train:\n",
    "    for l in lambdas: # train for all $\\lambda$ hyperparameters in 'lambdas' list\n",
    "        # for run in range(runs):\n",
    "        for run in runs:\n",
    "            print(f'training run: {run}, lambda: {l}')\n",
    "\n",
    "            # Define random_state. 0 for run=0, 1 for run=1, etc.\n",
    "            random_state = run\n",
    "            if random_state:\n",
    "                torch.manual_seed(random_state)\n",
    "                random.seed(random_state)\n",
    "                np.random.seed(random_state)\n",
    "\n",
    "            train_list, valid_list, test_list = get_split_data(dataset_type, data_path, train_data_cond, dataset_type, validation, random_state)\n",
    "            train_loader, valid_loader, test_loader, train_data, valid_data, test_data = get_loaders(dataset_type, model_type, train_list, valid_list, test_list, \n",
    "                                                                                                        batch_size, train_data_cond, test_data_cond)\n",
    "            model, criterion, optimizer = get_new_model_configs(vit_version, model_type, subset_layers, device)\n",
    "            trained_model_path_run = f'{trained_model_path}_{run}_{subset_layers}_layer_{l}_lambda'\n",
    "            with open(f'{trained_model_path}_{run}_{subset_layers}_{l}_datasets.pkl', 'wb') as handle:\n",
    "                pickle.dump([train_list, valid_list, test_list], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            early_stopper = EarlyStopper(patience=early_stopping_epochs, min_delta=10)\n",
    "            best_train_loss = 1e8\n",
    "            print('Starting training...')\n",
    "            for epoch in range(100):  # loop over the dataset multiple times\n",
    "                train_total_loss, train_accuracy = train_one_epoch(model_type, model, criterion, optimizer, train_loader, train_data, device, batch_size, l)\n",
    "                if validation:\n",
    "                    _, _, valid_accuracy, valid_total_loss = eval_dataset(model_type, model, criterion, valid_loader, valid_data, device, batch_size, l, export_preds = False)\n",
    "                else:\n",
    "                    valid_accuracy = np.NaN\n",
    "                    valid_total_loss = np.NaN\n",
    "                _, _, test_accuracy, test_total_loss = eval_dataset(model_type, model, criterion, test_loader, test_data, device, batch_size, l, export_preds = False)\n",
    "                \n",
    "                print(f'[epoch: {epoch + 1}] train_loss: {train_total_loss:.3f}, train_accuracy: {train_accuracy:.3f}%, \\\n",
    "                    valid_loss: {valid_total_loss:.3f}, valid_accuracy: {valid_accuracy:.3f}%, \\\n",
    "                        test_loss: {test_total_loss:.3f}, test_accuracy: {test_accuracy:.3f}%')\n",
    "                \n",
    "                if validation:\n",
    "                    if early_stopper.early_stop(valid_total_loss):    \n",
    "                        print(\"EARLY STOPPED\")         \n",
    "                        break\n",
    "                else:\n",
    "                    if early_stopper.early_stop(test_total_loss):    \n",
    "                        print(\"EARLY STOPPED\")         \n",
    "                        break\n",
    "                \n",
    "                col_vals = [vit_version, epoch, criterion, batch_size, train_total_loss, train_accuracy, valid_total_loss, valid_accuracy, test_total_loss, test_accuracy]\n",
    "                if epoch == 0: # overwrite existing\n",
    "                    col_headers = ['vit_version', 'epoch', 'criterion', 'batch_size', 'train_loss', 'train_accuracy', 'valid_loss', 'valid_accuracy', 'test_loss', 'test_accuracy']\n",
    "                    with open(f'{trained_model_path_run}.csv','w') as fd:\n",
    "                        writer = csv.writer(fd)\n",
    "                        writer.writerow(col_headers)\n",
    "                        writer.writerow(col_vals)\n",
    "                else:\n",
    "                    with open(f'{trained_model_path_run}.csv','a') as fd:\n",
    "                        writer = csv.writer(fd)\n",
    "                        writer.writerow(col_vals)\n",
    "\n",
    "                if early_stopper.counter == 0:\n",
    "                    print(f\"Saved model at {trained_model_path_run}_early_stopped.pt!\")\n",
    "                    torch.save(model.state_dict(), f'{trained_model_path_run}_early_stopped.pt')\n",
    "                else:\n",
    "                    print(f\"Saved model at {trained_model_path_run}_late.pt!\")\n",
    "                    torch.save(model.state_dict(), f'{trained_model_path_run}_late.pt')\n",
    "                best_train_loss = train_total_loss\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "else:\n",
    "    if model_type == 'jsf':\n",
    "        model = TimeSformer(img_size=224, num_classes=2, num_frames=2, attention_type='joint_space_time',  pretrained_model='').to(device)\n",
    "    else:\n",
    "        model = timm.create_model('vit_base_patch16_224', num_classes=2).to(device)\n",
    "        model.load_state_dict(torch.load(f'{trained_model_path}_{model_load_type}.pt', map_location=device))\n",
    "    if subset_layers:\n",
    "        model.blocks = nn.Sequential(*[model.blocks[i] for i in range(subset_layers)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnt_no_timm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
